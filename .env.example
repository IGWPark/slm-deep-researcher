# LLM Configuration
# Your LLM server URL (vLLM uses 8000 by default, Ollama uses 11434)
LLM_BASE_URL=http://localhost:8000/v1

# Leave empty if not required
LLM_API_KEY=

# Leave empty to auto-detect from server
LLM_MODEL=

LLM_TEMPERATURE=0.7
LLM_TOP_P=0.9
LLM_TOP_K=20
LLM_MIN_P=0.0

# Token Limits
LLM_MAX_INPUT_TOKENS=16384

# For initial query generation
LLM_MAX_OUTPUT_TOKENS_QUERY=512

# For summarizing search results
LLM_MAX_OUTPUT_TOKENS_SUMMARY=2048

# For reflection and follow-up queries
LLM_MAX_OUTPUT_TOKENS_REFLECTION=512

# Model Behavior
# Remove <think> tokens from reasoning models
LLM_STRIP_THINKING=true

# Use "tools" for tool calling, "json" for JSON mode
USE_TOOL_CALLING=tools

# Research Configuration
# How many research iterations to run
MAX_WEB_RESEARCH_LOOPS=3

# Search Configuration
FETCH_FULL_PAGE=true
DDGS_REGION=us-en
SEARCH_MAX_RESULTS=3
MAX_TOKENS_PER_SOURCE=1000
