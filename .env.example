# LLM Configuration
LLM_BASE_URL=http://localhost:8000/v1  # Your LLM server URL (vLLM uses 8000 by default, Ollama uses 11434)
LLM_API_KEY=                            # Leave empty if not required
LLM_MODEL=                              # Leave empty to auto-detect from server
LLM_TEMPERATURE=0.7
LLM_TOP_P=0.9
LLM_TOP_K=20
LLM_MIN_P=0.0

# Token Limits
LLM_MAX_INPUT_TOKENS=16384
LLM_MAX_OUTPUT_TOKENS_QUERY=512          # For initial query generation
LLM_MAX_OUTPUT_TOKENS_SUMMARY=2048       # For summarizing search results
LLM_MAX_OUTPUT_TOKENS_REFLECTION=512     # For reflection and follow-up queries

# Model Behavior
LLM_STRIP_THINKING=true                  # Remove <think> tokens from reasoning models
USE_TOOL_CALLING=tools                   # Use "tools" for tool calling, "json" for JSON mode

# Research Configuration
MAX_WEB_RESEARCH_LOOPS=3                 # How many research iterations to run

# Search Configuration
FETCH_FULL_PAGE=true                     # Fetch full page content via Docling
DDGS_REGION=us-en                        # DuckDuckGo region (us-en = US English)
SEARCH_MAX_RESULTS=3                     # Number of search results per query
MAX_TOKENS_PER_SOURCE=1000               # Token limit for each source content
